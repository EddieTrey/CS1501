# a3.md 
# image of original file sizes, compressed file sizes, and compression ratios:
    [screenshot](./ratios.png)

    
# Discussion Of The 4 LZW Variations 

1. Text Files and assig2.doc
    These files had compression rates of approximately 2-3. The reason upon speculation, is that text files often have repeated words. Due to the nature of LZW, repeated lengths of words accrue into longer lengths, and those long length of words are replaced by shorter codewords. This will result in higher compression ratios as the LZW file is much shorter than the original txt file. As for the differences between the LZW variations, the modded versions, including the one that resets the dictionary, had better ratios than the original LZW. This is because the modded versions both use stringbuilders instead of strings. This decreased the overhead cost because when strings are changed, java creates new instances of the string with the change. The modded version with dictionary resetting compared to the one without it, are very similar. I am guessing because the text in them do not use up all the codewords, thus the dictionary isn't reset as often. If the dictionary was reset more times, then the modded version would definitely have better compression ratios. The Unix version was very similar to the LZWmod version that resets the dictionary.

2. Tar Files and edit.exe
    According to https://fileinfo.com/extension/tar#:~:text=A%20TAR%20file%20is%20an,files%20with%20GNU%20Zip%20compression, tar files are large units that contain multiple uncompressed files. These files are mainly used for distribution or for backing up. This might be the reason why all the tar files had relatively great compression ratios. Since the files are all in the megabytes, they could very possibly use up all the codewords and require the dictionary to reset. Tar files have the luxury of including multiple files with different extensions. It is my guess that bmps.tar only has very similar files of the same type, that is why its compression ratio is so high, ranging from 9.72 - 13.89 (excluding the original LZW). One thing to note is that LZWmod with dictionary reset(13.89) had a better ratio than the non-reset version(9.72). The reason is because the tar files are large, thus the codewords are used up. When the dictionary resets, the codewords start fresh and so the compressed file is shorter by not writing char for char. The original LZW only had a compression rate of 1.16-1.63. I think it is because of its use of string instead of stringbuilder, causing much overhead that increased file size. Compress.exe was similar to LZWmod with reset.

    The edit.exe file had okay compression ratios(1.39-1.53) across the boards except when the original LZW tried to compress it. I am thinking it is also because of its string implementation.

3. Uncompressed Image Files(.bmps)
    This section talks about uncompressed files like bmps files. They all had good ratios(1.3-2.5), but were slightly worse than text files. This is because images have less duplicated data compared to words. Because the files are uncompressed, LZW provides benefits compressing them. The key thing to note here is that **wacky.bmp had the best compression ratio** of all test files. The reason is because it is mostly blank space. Blank space is duplicated data so LZW can use the same codeword to represent them. LZWmod, LZWmod with reset, and the Unix compress.exe all had the same compression ratio. Only the original LZW was different because, again, it uses string instead of stringbuilder.

4. Compressed Image Files (gif & jpg)
    These files had **the worst compression ratios of all the test files.** Apart from the compress.exe which was written by experts, all the LZW variations actually "compressed" the files into large ones. One reason, given by TA Gordan, is that they are already compressed. Compressed files don't provide much benefit to being compressed again, and they might even lose data if compressed twice. Another reason I can think of is that image files have much more arbitrary data versus text files. Together, these might cause the compression of already compressed files to become larger. LZW will simple replace unique data with new codewords each time, providing zero benefit. One thing I noticed was that the Unix compress.exe LZW had compression ratios of 1.0 for the two files. I think this is because the LZW was written smartly; so when it knew the files were already compressed, it didn't try to compress it again. I may be wrong, but that is my speculation.

5. Conclusion
    Overall what happened was the original LZW had the worse compression ratios, followed by LZWmod without dictionary resetting, then LZWmod with resets, and finally the Unix compression.exe LZW. This is largely expected because the original LZW uses strings which have large overheads when compressing larger files. The LZWmod without resets does a good job of compressing medium sized files, but as the codewords are used up, it loses effectiveness. The LZWmod with resets refreshes its codewords. So when big files are compressed, this variation starts to shine. Lastly is the expertly written Unix version. This variation outperforms the original LZW and also my modded versions slightly. It does especially well when compressing already-compressed image files because it does nothing to do them, while my versions still try to compress the files. Apart from those two image files, the Unix version's compression ratios compared to my modded variation with resets does better or worse in the following range :(-0.04 to +0.05). Since we used the same LZW algorithm, the performances are largely similar. Mine was even better for two of the files. :)
